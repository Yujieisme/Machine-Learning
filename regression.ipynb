{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一、机器学习的概念\n",
    "Supervised：\n",
    "For each observation of the predictor measurements xi, there is an associated response measurement yi.\n",
    "\n",
    "Unsupervised: \n",
    "Unsupervised learning describes the somewhat more challenging situation in which for every observation i, we observe a vector of measurements xi but no associated response yi.\n",
    "\n",
    "泛化能力：\n",
    "学得模型适用于新样本的能力，称为\"泛化\" (generalization) 能力\n",
    "\n",
    "交叉验证cross-validation:\n",
    "模型的Error = Bias + Variance，Error反映的是整个模型的准确度，Bias反映的是模型在样本上的输出与真实值之间的误差，即模型本身的精准度，Variance反映的是模型每一次输出结果与模型输出期望之间的误差，即模型的稳定性。\n",
    "\n",
    "欠拟合underfitting:\n",
    "对训练样本的一般性质尚未学好\n",
    "\n",
    "解决方法：\n",
    "1）添加其他特征项，有时候我们模型出现欠拟合的时候是因为特征项不够导致的，可以添加其他特征项来很好地解决。例如，“组合”、“泛化”、“相关性”三类特征是特征添加的重要手段，无论在什么场景，都可以照葫芦画瓢，总会得到意想不到的效果。除上面的特征之外，“上下文特征”、“平台特征”等等，都可以作为特征添加的首选项。\n",
    "\n",
    "2）添加多项式特征，这个在机器学习算法里面用的很普遍，例如将线性模型通过添加二次项或者三次项使模型泛化能力更强。例如上面的图片的例子。\n",
    "\n",
    "3）减少正则化参数，正则化的目的是用来防止过拟合的，但是现在模型出现了欠拟合，则需要减少正则化参数。\n",
    "\n",
    "过拟合overfitting:\n",
    "当学习器把训练样本学得\"太好\"了的时候，很可能巳经把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质，这样就会导致泛化性能下降这种现象在机器学习中称为\"过拟合\" (overfitting).随着模型训练的进行，模型的复杂度会增加，此时模型在训练数据集上的训练误差会逐渐减小，但是在模型的复杂度达到一定程度时，模型在验证集上的误差反而随着模型的复杂度增加而增大。此时便发生了过拟合，即模型的复杂度升高，但是该模型在除训练集之外的数据集上却不work\n",
    "\n",
    "解决方法：\n",
    "1）重新清洗数据，导致过拟合的一个原因也有可能是数据不纯导致的，如果出现了过拟合就需要我们重新清洗数据。\n",
    "\n",
    "2）增大数据的训练量，还有一个原因就是我们用于训练的数据量太小导致的，训练数据占总数据的比例过小。\n",
    "\n",
    "3）采用正则化方法。正则化方法包括L0正则、L1正则和L2正则，而正则一般是在目标函数之后加上对于的范数。但是在机器学习中一般使用L2正则，下面看具体的原因。\n",
    "\n",
    "L0范数是指向量中非0的元素的个数。L1范数是指向量中各个元素绝对值之和，也叫“稀疏规则算子”（Lasso regularization）。两者都可以实现稀疏性，既然L0可以实现稀疏，为什么不用L0，而要用L1呢？个人理解一是因为L0范数很难优化求解（NP难问题），二是L1范数是L0范数的最优凸近似，而且它比L0范数要容易优化求解。所以大家才把目光和万千宠爱转于L1范数。\n",
    "\n",
    "L2范数是指向量各元素的平方和然后求平方根。可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0。L2正则项起到使得参数w变小加剧的效果，但是为什么可以防止过拟合呢？一个通俗的理解便是：更小的参数值w意味着模型的复杂度更低，对训练数据的拟合刚刚好（奥卡姆剃刀），不会过分拟合训练数据，从而使得不会过拟合，以提高模型的泛化能力。还有就是看到有人说L2范数有助于处理 condition number不好的情况下矩阵求逆很困难的问题（具体这儿我也不是太理解）。\n",
    "4）采用dropout方法。这个方法在神经网络里面很常用。dropout方法是ImageNet中提出的一种方法，通俗一点讲就是dropout方法在训练的时候让神经元以一定的概率不工作。\n",
    "\n",
    "二、线性回归\n",
    "线性回归原理：\n",
    "线性回归遇到的问题一般是这样的。我们有m个样本，每个样本对应于n维特征和一个结果输出，如下：\n",
    "\n",
    "　　　　(x(0)1,x(0)2,...x(0)n,y0),(x(1)1,x(1)2,...x(1)n,y1),...(x(m)1,x(m)2,...x(m)n,ym)\n",
    "　　　　我们的问题是，对于一个新的(x(x)1,x(x)2,...x(x)n, 他所对应的yx是多少呢？ 如果这个问题里面的y是连续的，则是一个回归问题，否则是一个分类问题。\n",
    "\n",
    "　　　　对于n维特征的样本数据，如果我们决定使用线性回归，那么对应的模型是这样的：\n",
    "\n",
    "　　　　hθ(x1,x2,...xn)=θ0+θ1x1+...+θnxn, 其中θi (i = 0,1,2... n)为模型参数，xi (i = 0,1,2... n)为每个样本的n个特征值。这个表示可以简化，我们增加一个特征x0=1 ，这样hθ(x0,x1,...xn)=∑i=0nθixi。\n",
    "\n",
    "　　　　进一步用矩阵形式表达更加简洁如下：\n",
    "\n",
    "　　　　hθ(X)=Xθ\n",
    "　　　　其中， 假设函数hθ(X)为mx1的向量,θ为nx1的向量，里面有n个代数法的模型参数。X为mxn维的矩阵。m代表样本的个数，n代表样本的特征数。\n",
    "\n",
    "　　　　得到了模型，我们需要求出需要的损失函数，一般线性回归我们用均方误差作为损失函数。损失函数的代数法表示如下：\n",
    "\n",
    "　　　　J(θ0,θ1...,θn)=∑i=0m(hθ(x0,x1,...xn)−yi)2\n",
    "　　　　进一步用矩阵形式表达损失函数：\n",
    "\n",
    "　　　　J(θ)=12(Xθ−Y)T(Xθ−Y)\n",
    "　　　　由于矩阵法表达比较的简洁，后面我们将统一采用矩阵方式表达模型函数和损失函数。\n",
    "    \n",
    "线性回归损失函数、代价函数：\n",
    "\n",
    "损失函数：\n",
    "J(θ0,θ1...,θn)=∑i=0m(hθ(x0,x1,...xn)−yi)2\n",
    "J(θ)=12(Xθ−Y)T(Xθ−Y)\n",
    "\n",
    "代价函数：\n",
    "![title](01.png)\n",
    "\n",
    "一元线性回归的参数求解公式推导：\n",
    "![title](03.jpg)\n",
    "\n",
    "多元线性回归的参数求解公式推导：\n",
    "![title](02.jpg)\n",
    "\n",
    "sklearn参数：\n",
    "实例化\n",
    "sklearn一直秉承着简洁为美得思想设计着估计器，实例化的方式很简单，使用clf = LinearRegression()就可以完成，但是仍然推荐看一下几个可能会用到的参数：\n",
    "\n",
    "fit_intercept：是否存在截距，默认存在\n",
    "normalize：标准化开关，默认关闭\n",
    "还有一些参数感觉不是太有用，就不再说明了，可以去官网文档中查看。\n",
    "\n",
    "回归\n",
    "其实在上面的例子中已经使用了fit进行回归计算了，使用的方法也是相当的简单。\n",
    "\n",
    "fit(X,y,sample_weight=None)：X,y以矩阵的方式传入，而sample_weight则是每条测试数据的权重，同样以array格式传入。\n",
    "predict(X)：预测方法，将返回预测值y_pred\n",
    "score(X,y,sample_weight=None)：评分函数，将返回一个小于1的得分，可能会小于0\n",
    "方程\n",
    "LinearRegression将方程分为两个部分存放，coef_存放回归系数，intercept_则存放截距，因此要查看方程，就是查看这两个变量的取值。\n",
    "\n",
    "多项式回归\n",
    "其实，多项式就是多元回归的一个变种，只不过是原来需要传入的是X向量，而多项式则只要一个x值就行。通过将x扩展为指定阶数的向量，就可以使用LinearRegression进行回归了。sklearn已经提供了扩展的方法——sklearn.preprocessing.PolynomialFeatures。利用这个类可以轻松的将x扩展为X向量\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
